\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}

% Configuración de geometría
\geometry{top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm}

% Configuración de encabezados y pies de página
\setlength{\headheight}{15pt}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Análisis Comparativo de Algoritmos de Clustering}
\fancyhead[R]{K-means vs SOM}
\fancyfoot[C]{\thepage}

% Configuración de hyperref
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

% Configuración de colores
\definecolor{lightblue}{RGB}{173,216,230}
\definecolor{lightgreen}{RGB}{144,238,144}
\definecolor{lightyellow}{RGB}{255,255,224}

\title{\textbf{Análisis Comparativo de Algoritmos de Clustering:\\ K-means vs Self-Organizing Maps (SOM)}}
\author{
    \textbf{Taller No. 1 - Aprendizaje No Supervisado}\\
    \textit{Programa de Posgrado en Big Data}\\
    Universidad [Nombre de la Universidad]
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Este informe presenta un análisis comparativo exhaustivo entre los algoritmos de clustering K-means y Self-Organizing Maps (SOM) aplicados a dos conjuntos de datos con diferentes niveles de complejidad. Se implementó una metodología semi-supervisada con validación cruzada para evaluar la efectividad de ambos algoritmos en tareas de clasificación. Los resultados demuestran que SOM supera significativamente a K-means en datasets complejos multi-clase (45.79\% vs 33.10\% de accuracy), mientras que K-means mantiene competitividad en problemas simples con clusters bien definidos. El análisis incluye preprocesamiento avanzado con balanceamiento híbrido, optimización de hiperparámetros, y evaluación mediante múltiples métricas cuantitativas.
\end{abstract}

\tableofcontents
\newpage

\section{Introducción}

El clustering es una técnica fundamental en el aprendizaje no supervisado que busca agrupar datos similares sin conocimiento previo de las etiquetas. En este trabajo se comparan dos algoritmos representativos: K-means, basado en centroides, y Self-Organizing Maps (SOM), basado en redes neuronales competitivas.

\subsection{Objetivos}

\subsubsection{Objetivo General}
Evaluar y comparar el rendimiento de los algoritmos K-means y SOM en diferentes tipos de datasets, utilizando una metodología semi-supervisada con validación cruzada.

\subsubsection{Objetivos Específicos}
\begin{itemize}
    \item Identificar configuraciones óptimas para ambos algoritmos en datasets simples y complejos
    \item Implementar una metodología semi-supervisada para evaluación de clustering
    \item Analizar la efectividad del balanceamiento de clases en datasets desequilibrados
    \item Comparar algoritmos mediante múltiples métricas cuantitativas
    \item Proporcionar recomendaciones prácticas para la selección de algoritmos
\end{itemize}

\subsection{Hipótesis}
Se plantea que SOM demostrará superioridad en datasets complejos multi-dimensionales debido a su capacidad de preservar la topología del espacio de características, mientras que K-means será más efectivo en datasets simples con clusters esféricos bien definidos.

\section{Marco Teórico}

\subsection{K-means}
K-means es un algoritmo de particionamiento que busca dividir los datos en $k$ clusters minimizando la suma de distancias cuadráticas intra-cluster:

\begin{equation}
J = \sum_{i=1}^{k} \sum_{x \in C_i} ||x - \mu_i||^2
\end{equation}

donde $\mu_i$ es el centroide del cluster $C_i$.

\subsection{Self-Organizing Maps (SOM)}
SOM es una red neuronal no supervisada que proyecta datos de alta dimensión en un mapa topológico de menor dimensión, típicamente bidimensional. La función de vecindad se define como:

\begin{equation}
h_{c,i}(t) = \alpha(t) \cdot \exp\left(-\frac{||r_c - r_i||^2}{2\sigma^2(t)}\right)
\end{equation}

donde $\alpha(t)$ es la tasa de aprendizaje y $\sigma(t)$ el radio de vecindad.

\section{Metodología}

\subsection{Datasets Utilizados}

\subsubsection{Dataset 1: data\_clusters.mat}
\begin{itemize}
    \item \textbf{Características}: 136 muestras, 2 dimensiones
    \item \textbf{Tipo}: Dataset sintético con clusters bien definidos
    \item \textbf{Propósito}: Evaluación en problema simple
\end{itemize}

\subsubsection{Dataset 2: Avila UCI}
\begin{itemize}
    \item \textbf{Características}: 20,867 muestras originales, 10 dimensiones, 8 clases
    \item \textbf{Tipo}: Dataset real de clasificación paleográfica
    \item \textbf{Propósito}: Evaluación en problema complejo multi-clase
    \item \textbf{Descripción}: Clasificación de copistas de manuscritos del siglo XII
\end{itemize}

\subsection{Preprocesamiento}

\subsubsection{Balanceamiento Híbrido (Dataset Avila)}
Se implementó una estrategia en dos fases para corregir el desequilibrio de clases (ratio original 12.16:1):

\begin{enumerate}
    \item \textbf{Fase 1 - Undersampling}: Reducción de la clase mayoritaria (A) de 8,572 a 4,500 muestras
    \item \textbf{Fase 2 - SMOTE}: Generación sintética para elevar clases minoritarias a ~2,500 muestras cada una
\end{enumerate}

\textbf{Resultado}: 23,423 muestras balanceadas con ratio 1.80:1 (mejora de 6.8x).

\subsubsection{Normalización}
\begin{itemize}
    \item \textbf{K-means}: StandardScaler (media=0, desviación=1)
    \item \textbf{SOM}: MinMaxScaler (rango [0,1]) posterior a StandardScaler
\end{itemize}

\subsection{División de Datos}
Se implementó división estratificada 70/15/15:
\begin{itemize}
    \item \textbf{70\% Entrenamiento}: Clustering no supervisado
    \item \textbf{15\% Etiquetado}: Mapeo cluster → clase
    \item \textbf{15\% Validación}: Evaluación final
\end{itemize}

\subsection{Configuraciones Evaluadas}

\subsubsection{K-means}
\begin{itemize}
    \item \textbf{Dataset simple}: K $\in$ [2, 10] (9 configuraciones)
    \item \textbf{Dataset Avila}: K $\in$ [2, 20] (19 configuraciones, 5 ejecuciones c/u)
    \item \textbf{Métricas}: Método del codo, coeficiente de silhouette
\end{itemize}

\subsubsection{SOM}
\begin{itemize}
    \item \textbf{Dataset simple}: Rejillas 3$\times$3 hasta 8$\times$8 (15 configuraciones)
    \item \textbf{Dataset Avila}: Rejillas 3$\times$3 hasta 10$\times$10 (22 configuraciones)
    \item \textbf{Métricas}: Error de cuantización, eficiencia neuronal
\end{itemize}

\section{Resultados}

\subsection{Dataset Simple (data\_clusters.mat)}

\subsubsection{Configuraciones Óptimas}
\begin{table}[H]
\centering
\caption{Resultados Óptimos - Dataset Simple}
\begin{tabular}{lcc}
\toprule
\textbf{Algoritmo} & \textbf{Configuración} & \textbf{Métrica Principal} \\
\midrule
K-means & K=6 & Silhouette = 0.7396 \\
SOM & 7×7 (35 activas) & Error cuant. = 0.0241 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Análisis Comparativo}
K-means demostró ligera superioridad en el dataset simple debido a:
\begin{itemize}
    \item Clusters esféricos bien definidos
    \item Baja dimensionalidad (2D)
    \item Ausencia de ruido significativo
\end{itemize}

\subsection{Dataset Complejo (Avila UCI)}

\subsubsection{Configuraciones Óptimas}
\begin{table}[H]
\centering
\caption{Resultados Óptimos - Dataset Avila}
\begin{tabular}{lcc}
\toprule
\textbf{Algoritmo} & \textbf{Configuración} & \textbf{Métrica Principal} \\
\midrule
K-means & K=10 & Silhouette = 0.2176 ± 0.0283 \\
SOM & 8×8 (63 activas) & Error cuant. = 0.1068 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Resultados de Validación Cruzada}
\begin{table}[H]
\centering
\caption{Rendimiento Semi-supervisado - Validación Cruzada 5-fold}
\begin{tabular}{lcc}
\toprule
\textbf{Algoritmo} & \textbf{Accuracy CV} & \textbf{Accuracy Final} \\
\midrule
K-means & 0.2955 ± 0.0304 & 33.10\% \\
SOM & 0.4214 ± 0.0102 & 45.79\% \\
\midrule
\textbf{Ventaja SOM} & \textbf{+42.6\%} & \textbf{+38.2\%} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Análisis de Calidad de Etiquetado}

Se evaluó la calidad del proceso de etiquetado midiendo la pureza de cada cluster/neurona:

\begin{table}[H]
\centering
\caption{Calidad del Etiquetado}
\begin{tabular}{lcccc}
\toprule
\textbf{Métrica} & \textbf{K-means} & \textbf{SOM} & \textbf{Ganador} \\
\midrule
Pureza Promedio & 39.7\% & 50.5\% & SOM \\
Pureza Mediana & 36.2\% & 40.7\% & SOM \\
Pureza Mínima & 20.1\% & 21.3\% & SOM \\
Pureza Máxima & 91.0\% & 100.0\% & SOM \\
Desv. Std. Pureza & 20.1\% & 23.7\% & K-means \\
\midrule
\textbf{Métricas Ganadas} & \textbf{1/5} & \textbf{4/5} & \textbf{SOM} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Matrices de Confusión}

Las matrices de confusión revelan patrones importantes en el rendimiento:

\subsubsection{K-means}
\begin{itemize}
    \item Fuerte sesgo hacia clasificación como clase F
    \item Recall prácticamente nulo para clases D, E, G, H
    \item Mejor rendimiento solo en clase I (recall: 69\%)
\end{itemize}

\subsubsection{SOM}
\begin{itemize}
    \item Distribución más equilibrada entre clases
    \item Mejor recall promedio (45\% vs 31\%)
    \item Rendimiento superior en 6 de 8 clases
\end{itemize}

\subsection{Análisis de Eficiencia Computacional}

\begin{table}[H]
\centering
\caption{Comparación de Eficiencia}
\begin{tabular}{lcc}
\toprule
\textbf{Aspecto} & \textbf{K-means} & \textbf{SOM} \\
\midrule
Tiempo entrenamiento & Rápido & Moderado \\
Escalabilidad & Excelente & Buena \\
Memoria requerida & Baja & Moderada \\
Interpretabilidad & Alta & Media \\
Robustez a ruido & Media & Alta \\
\bottomrule
\end{tabular}
\end{table}

\section{Discusión}

\subsection{¿Por qué SOM Superó a K-means en el Dataset Complejo?}

\subsubsection{Preservación de Topología}
SOM mantiene las relaciones de vecindad del espacio original, crucial para datos paleográficos donde características similares deben agruparse próximamente.

\subsubsection{Mejor Representación}
Las 63 neuronas activas del SOM 8×8 capturan patrones más sutiles que los 10 centroides de K-means, permitiendo una segmentación más granular.

\subsubsection{Flexibilidad Estructural}
La rejilla bidimensional permite representar distribuciones complejas y no esféricas, comunes en datos reales de alta dimensión.

\subsubsection{Menor Sensibilidad}
SOM es menos afectado por outliers y ruido debido a su mecanismo de aprendizaje competitivo y función de vecindad.

\subsection{Efectividad del Balanceamiento}

El balanceamiento híbrido demostró ser crucial:
\begin{itemize}
    \item Mejora del ratio de 12.16:1 a 1.80:1 (6.8× mejor)
    \item Reducción de desviación estándar entre clases (3.3× menor)
    \item Validación cruzada más estable
    \item Matrices de confusión más distribuidas
\end{itemize}

\subsection{Implicaciones para Clasificación Paleográfica}

En el contexto de clasificación de copistas manuscritos:
\begin{itemize}
    \item \textbf{SOM} ofrece mejor interpretabilidad topológica
    \item Permite identificar estilos de escritura similares mediante vecindad
    \item La preservación de estructura es crucial para análisis paleográfico
    \item La granularidad de 63 neuronas vs 10 clusters mejora discriminación
\end{itemize}

\section{Conclusiones}

\subsection{Conclusiones Principales}

\begin{enumerate}
    \item \textbf{Superioridad contextual}: SOM demuestra superioridad significativa en datasets complejos multi-clase (45.79\% vs 33.10\%), mientras K-means mantiene competitividad en problemas simples.
    
    \item \textbf{Efectividad del balanceamiento}: La estrategia híbrida (undersampling + SMOTE) es crucial para datasets desequilibrados, mejorando el rendimiento de ambos algoritmos.
    
    \item \textbf{Calidad de etiquetado}: SOM genera clusters con mayor pureza promedio (50.5\% vs 39.7\%) y mejor distribución de clases.
    
    \item \textbf{Metodología semi-supervisada}: La división 70/15/15 con validación cruzada proporciona evaluación robusta y metodológicamente sólida.
\end{enumerate}

\subsection{Recomendaciones Prácticas}

\subsubsection{Usar K-means cuando:}
\begin{itemize}
    \item Datos tienen clusters aproximadamente esféricos
    \item Se requiere interpretabilidad simple (centroides)
    \item Eficiencia computacional es prioritaria
    \item Dimensionalidad baja-media (<10 características)
\end{itemize}

\subsubsection{Usar SOM cuando:}
\begin{itemize}
    \item Datos tienen estructura topológica compleja
    \item Se requiere preservación de vecindad
    \item Visualización de alta dimensionalidad es importante
    \item Se necesita detectar patrones sutiles
\end{itemize}

\subsection{Configuraciones Recomendadas}

\begin{table}[H]
\centering
\caption{Configuraciones Recomendadas por Tipo de Problema}
\begin{tabular}{lcc}
\toprule
\textbf{Tipo de Problema} & \textbf{K-means} & \textbf{SOM} \\
\midrule
Dataset Simple (2D) & K=6 & 7×7 \\
Dataset Complejo (>5D) & K=10 & 8×8 \\
Clasificación Paleográfica & No recomendado & 8×8 \\
\bottomrule
\end{tabular}
\end{table}

\section{Trabajo Futuro}

\subsection{Algoritmos Avanzados}
\begin{itemize}
    \item Evaluación de DBSCAN y Gaussian Mixture Models
    \item Implementación de autoencoders para clustering
    \item Clustering ensemble combinando múltiples algoritmos
\end{itemize}

\subsection{Optimización}
\begin{itemize}
    \item Búsqueda automática de hiperparámetros con Optuna
    \item Paralelización para datasets masivos
    \item Implementación en GPU para SOM
\end{itemize}

\subsection{Aplicaciones}
\begin{itemize}
    \item Extensión a otros dominios paleográficos
    \item Aplicación en clasificación de documentos históricos
    \item Desarrollo de sistema interactivo de análisis
\end{itemize}

\section{Referencias}

\begin{enumerate}
    \item Kohonen, T. (2001). \textit{Self-Organizing Maps}. Springer-Verlag.
    \item MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. \textit{Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability}, 1, 281-297.
    \item Chawla, N. V., et al. (2002). SMOTE: synthetic minority over-sampling technique. \textit{Journal of artificial intelligence research}, 16, 321-357.
    \item Rousseeuw, P. J. (1987). Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. \textit{Journal of computational and applied mathematics}, 20, 53-65.
    \item De Stefano, C., et al. (2018). Reliable writer identification in medieval manuscripts through page layout features: The "Avila" Bible case. \textit{Engineering Applications of Artificial Intelligence}, 72, 99-110.
\end{enumerate}

\appendix

\section{Código Principal}

\begin{lstlisting}[language=Python, caption=Implementación K-means Optimizado]
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Configuracion K-means
k_range = range(2, 21)
best_silhouette = -1
best_k = 2

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X_train_scaled)
    silhouette_avg = silhouette_score(X_train_scaled, cluster_labels)
    
    if silhouette_avg > best_silhouette:
        best_silhouette = silhouette_avg
        best_k = k
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Implementación SOM Optimizado]
from minisom import MiniSom

# Configuracion SOM
som_configs = []
for x in range(3, 11):
    for y in range(3, 11):
        som_configs.append({'x': x, 'y': y})

best_som = None
best_score = float('inf')

for config in som_configs:
    som = MiniSom(config['x'], config['y'], X_train_scaled.shape[1],
                  sigma=1.0, learning_rate=0.5, random_seed=42)
    som.train(X_train_scaled, 1000)
    
    # Calcular error de cuantizacion
    quantization_error = som.quantization_error(X_train_scaled)
    
    if quantization_error < best_score:
        best_score = quantization_error
        best_som = som
\end{lstlisting}

\section{Métricas Detalladas}

\subsection{Resultados Completos Dataset Avila}

\begin{table}[H]
\centering
\scriptsize
\caption{Resultados Detallados por Clase - Dataset Avila}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{\textbf{Clase}} & \multicolumn{3}{c}{\textbf{K-means}} & \multicolumn{3}{c}{\textbf{SOM}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
& \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
\midrule
A & 0.27 & 0.36 & 0.31 & 0.37 & 0.49 & 0.42 \\
D & 0.00 & 0.00 & 0.00 & 0.40 & 0.23 & 0.29 \\
E & 0.00 & 0.00 & 0.00 & 0.37 & 0.35 & 0.36 \\
F & 0.25 & 0.60 & 0.35 & 0.37 & 0.53 & 0.43 \\
G & 0.00 & 0.00 & 0.00 & 0.35 & 0.18 & 0.24 \\
H & 0.20 & 0.17 & 0.18 & 0.35 & 0.26 & 0.30 \\
I & 0.89 & 0.69 & 0.77 & 0.85 & 0.78 & 0.82 \\
X & 0.43 & 0.67 & 0.52 & 0.80 & 0.77 & 0.78 \\
\midrule
\textbf{Promedio} & \textbf{0.25} & \textbf{0.31} & \textbf{0.27} & \textbf{0.48} & \textbf{0.45} & \textbf{0.45} \\
\bottomrule
\end{tabular}
\end{table}

\end{document}